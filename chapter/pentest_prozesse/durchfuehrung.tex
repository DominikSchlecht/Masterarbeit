\section{Durchführung}
Eine gute Vorbereitung vereinfacht in vielerlei Hinsicht die Durchführung von Pen-Test - beispielsweise insofern, dass bereits funktionierende Hard- und Software zur Verfügung steht und während des Pen-Tests somit keine Zeit auf Update und Konfiguration verwendet werden muss. Trotzdem gibt es einige bewährte Vorgehensweisen, welche man in Prozessen festhalten oder auf welche man sich einigen sollte. 

	\subsection{Kickoff}\label{ref:PenProzKickoffWeb}
	Unmittelbar vor dem Pen-Test sollte ein Kickoff durchgeführt werden. Zu diesem sollten alle verantwortlichen Stellen eingeladen werden (Business-Unit Manager, System-Administrator, der Sicherheitsverantwortliche des Projekts, eventuell Informationssicherheit und Hosting-Provider) und Kern-Fragen abgesprochen werden.\\
	
	Im Rahmen dieser Arbeit wurden die folgenden Fragen definiert und mit Ansprechpartnern der Allianz Deutschland AG geprüft. Diese sollten im Kickoff bearbeitet werden.
			
\paragraph{Allgemein}
\begin{itemize}
	\item Wie ist der Projekt-Name?
	\item Wer sind die Teilnehmer?
	\item Wann soll der Test durchgeführt werden?
	\item Was ist das Ziel des Tests?
	\item In welcher Stage befindet sich die Anwendung? Development Test System Integration Produktion
	\item Welche IP-Adressen sollen getestet werden?
	\item Welche URLs sollten getestet werden?
	\item Welche Zugangsdaten sollen genutzt werden?
	\item Sollen Denial-Of-Service-Angriffe durchgeführt werden?\\
	(Ja/Nein)
	\item Liegt ein Haftungsauschschluss vor?\\
	(Ja/Nein)
	\item Liegt eine Erlaubnis des Server-Betreibers vor?\\
	(Ja/Nein)
	\item Gibt es eine Deadline für den Bericht?\\
	(Ja/Nein)
	\item Erste Ergebnisse am Ende des Pen-Tests in einfacher Form zukommen lassen (z.B. Excel)?\\
	(Ja/Nein)
\end{itemize}


\paragraph{Fragen an den System-Administrator}
\begin{itemize}
\item Gibt es Systeme, die als instabil angesehen werden (alte Patch-Stände, Legacy Systeme etc.)?\\
(Ja/Nein)
\item Gibt es Systeme von Dritten, die ausgeschlossen werden müssen oder für die weitere Genehmigungen notwendig sind?\\
(Ja/Nein)
\item Was ist die Durchschnittszeit zur Wiederherstellung der Funktionalität eines Services?
\item Ist eine Monitoring-Software im Einsatz?\\
(Ja/Nein)
\item Welche sind die kritischsten Applikationen?
\item Werden in einem regelmäßigen Turnus Backups erstellt und getestet?\\
(Ja/Nein)
\end{itemize}

\paragraph{Fragen an den Business Unit Manager}
\begin{itemize}
\item Ist die Führungsebene über den Test informiert?\\
(Ja/Nein)
\item Welche Daten stellen das größte Risiko dar, falls diese manipuliert werden?
\item Gibt es Testfälle, die die Funktionalität der Services prüfen und belegen können?\\
(Ja/Nein)
\item Sind "`Disaster Recovery Procedures"' vorhanden?\\
(Ja/Nein)
\end{itemize}

\paragraph{Abschluss}
\begin{itemize}
\item Offene TODOs
\end{itemize}
	
		Um den Kickoff möglichst effizient zu gestalten, wurden auch diese Fragen über die gleiche Technik wie bei \ref{ref:AufImplInWeb} in eine Webanwendung integriert. Diese ist auf dem Datenträger unter "`pentest\_helper"' abgelegt.
			
	\subsection{Kategorisierung von Findings}
	Um einen besseren Überblick über Findings zu bekommen, werden diese meist in Kategorien eingeteilt. Im Folgenden werden zwei Methoden, die \textit{OWASP TOP 10} und die \textit{CWE} (\textit{Common Weakness Enumeration}), vorgestellt.
	
		\subsubsection{OWASP TOP 10}
		Die \textit{OWASP Foundation} (Open Web Application Security Project) ist eine non-Profit Organisation, welche es sich zum Ziel gemacht hat, das Schreiben sicherer Software für Unternehmen einfacher zu gestalten.\\
		
		Im Rahmen des Projekts erfasst die \textit{OWASP} alle 3 Jahre die am meisten aufgetretenen Schwachstellen und veröffentlicht die sogenannte \textit{OWASP TOP 10}. Da die \textit{TOP 10} aus dem Jahr 2016 zu diesem Zeitpunkt noch nicht veröffentlicht sind, ist die akutelle Version aus dem Jahr 2013. Aus dieser ergeben sich folgende Kategorien\cite{OWASPTOP10}:
		\begin{itemize}
			\item A1-Injection
			\item A2-Broken Authentication and Session Management
			\item A3-Cross-Site Scripting (XSS)
			\item A4-Insecure Direct Object References
			\item A5-Security Misconfiguration
			\item A6-Sensitive Data Exposure
			\item A7-Missing Function Level Access Control
			\item A8-Cross-Site Request Forgery (CSRF)
			\item A9-Using Components with Known Vulnerabilities 
			\item A10-Unvalidated Redirects and Forwards
		\end{itemize}
		
		Diese sind in der Security-Szene weit verbreitet und können gut genutzt werden, um Pen-Test-Findings zu kategorisieren.
		
		\subsubsection{Common Weakness Enumeration}
		Eine Alternative ist die von der MITRE Corperation (mit Unterstützung verschiedener andere Stellen) entwickelte \textit{Common Weakness Enumeration}. Diese ist wesentlich feingranularer als die \textit{OWASP TOP 10}. So umfasst die Version 2.10 vom 19.01.2017 über 1000 verschiedene Schwachstellen.\footnote{\url{http://cwe.mitre.org/data/published/cwe_v2.10.pdf}}\cite{MITRECWE}\\
		
		Dies hat den Vorteil, dass man die Schwachstellen meist genau einer Kategorie zuweisen kann. Jedoch ist die Zuordnung wesentlich aufwändiger.
			
	\subsection{Bewertung von Findings}
	Für die in einem Pen-Test gefundenen Findings sollte direkt nach deren Entdeckung eine Bewertung durchgeführt werden. Dies ist notwendig, um entsprechende Eskalationsstufen zu informieren, wenn kritische Findings auftreten. Zur Bewertung bieten sich verschiedene Systeme an; im Folgenden werden \textit{CVSS} und \textit{DREAD} vorgestellt und verglichen.
	
			\subsubsection{CVSS}
			\textit{FIRST.Org, Inc.}, eine in Amerika ansässige wohltätige Organisation, welche Lösungen zur Koordination und Unterstützung von IT-Security-Teams entwickelt, ist sowohl Eigentümer als auch Entwickler des \textit{CVSS} (\textit{Common Vulnerability Scoring System}).\\
			
			Das \textit{CVSS} bestimmt anhand mehrerer Attribute die Charakteristika und Risiko-Stufen von Schwachstellen. Dabei gibt es 3 Metriken: \textit{Base}, \textit{Temporal} und \textit{Environmental}. Die \textit{Base}-Metrik beschreibt den Grund-Score einer Schwachstelle, die \textit{Temporal}-Metrik der Score zum aktuellen Zeitpunkt und die  \textit{Environmental}-Metrik den Score in einem bestimmten Umfeld. Der Base-Score ist verpflichtend auszufüllen, die anderen Metriken können das Ergebnis verfeinern, sind aber nicht zwingend notwendig.\cite{FIRSTCVSS}\\
			
			Die \textit{Base}-Metrik ergibt einen Score zwischen 0.0 und 10.0, welcher durch die beiden Zusatz-Metriken erhöht oder geschwächt werden kann. Eine komprimierte Darstellungsweise stellt der \textit{CVSS-Vector-String} dar, welcher die Attribute aus allen drei Metriken sowie deren jeweiligen Werte komprimiert anzeigt. Die Attribute für die \textit{Base}-Metrik sind im Vergleich unter \ref{ref:VerglCVSS3} dargestellt. Die Attribute für die \textit{Temporal}- und \textit{Environmental}-Metrik sowie die genauen Formeln können der Spezifikation\footnote{\url{https://www.first.org/cvss/specification-document}} entnommen werden.\cite{FIRSTCVSS}\\
		
		Zusätzlich gib es für den Score eine Zuordnung zu 5 gröberen Risikostufen. Diese sind der Tabelle \ref{tab:cvssToRisk} zu entnehmen.\\
		
		\begin{table}
			\centering
			\begin{tabularx}{5cm}{l | l}
				Risikostufe & CVSS3-Score \\\hline
				None & 0.0 \\
				Low & 0.1 - 3.9 \\
				Medium & 4.0 - 6.9 \\
				High & 7.0 - 8.9 \\
				Critical & 9.0 - 10.0	 \\
			\end{tabularx}
			\caption{Zuordnung von CVSS3-Score zur Risikostufe
			\label{tab:cvssToRisk}
 \cite{FIRSTCVSSSpec}}
		\end{table}
		
		Ebenso gibt es \textit{CVSS}-Module für verschiedene Programmiersprachen, zum Beispiel \textit{Python}\footnote{\url{https://pypi.python.org/pypi/cvss}}. Über diese kann ein\textit{ CVSS-Vector-String} ausgewertet und die Scores für die einzelnen Metriken berechnet werden.

		\subsubsection{DREAD}
		
		\textit{DREAD} wie \textit{CVSS} ein Metrik-System zur Einschätzung des resultierenden Risikos aus einer Schwachstelle. \textit{DREAD} ist dabei das Akronym für die fünf bewerteten Attribute der Schwachstelle. Jedes Attribut hat einen Wert von 0 bis 10, wobei 10 immer der schlimmste anzunehmende Fall ist. Im Folgenden sind die Attribute sowie grobe Richtwerte pro Attribut aufgeführt.\cite{DREADOWASP}
		
		\begin{description}
			\item[Damage: ] Wie viel Schaden würde eine Ausnutzung der Schwachstelle bedeuten?
			\begin{itemize}
				\item[0] Kein Schaden
				\item[5] Die Daten eines einzelnen Users sind betroffen
				\item[10] Komplette Zerstörung der Daten oder des Systems 
			\end{itemize}
			
			\item[Reproducibility: ] Wie verlässlich funktioniert der Exploit?
			\begin{itemize}
				\item[0] Selbst mit erhöhten Rechten ist ein funktionierender Exploit äußerst unwahrscheinlich
				\item[5] Mehrstufiges Vorgehen notwendig, es gibt vorgefertigte Scripte oder Tools
				\item[10] Nicht authentifizierte User können den Exploit trivial ohne Hilfsmittel reproduzierbar durchführen
			\end{itemize}
			
			\item[Exploitability: ] Wie schwer ist es, die Schwachstelle auszunutzen?
			\begin{itemize}
				\item[0] Darf nicht vergeben werden, da angenommen wird, dass jede Schwachstelle mit genügend Aufwand exploitable ist
				\item[1] Selbst mit direktem Wissen der Schwachstelle gibt derzeit keine bekannte Methode zur Ausnutzung
				\item[5] Der Exploit ist öffentlich, mittleres Können durch den Angreifer wird benötigt, Angreifer muss authentisiert sein
				\item[7] Der Exploit ist öffentlich, Angreifer muss nicht authentisiert sein
				\item[10] Triviale Ausnutzung, zum Beispiel über einen Webbrowser
			\end{itemize}
			
			\item[Affected Users: ] Wie viele Nutzer betrifft die Schwachstelle?
			\begin{itemize}
				\item[0] Keine User betroffen
				\item[5] Einige User betroffen, aber nicht alle
				\item[10] Alle User betroffen
			\end{itemize}
			
			\item[Discoverability: ] Wie einfach ist die Schwachstelle zu finden?
			\begin{itemize}
				\item[0] Selbst mit Source-Code-Zugriff und erhöhten Rechten schwer zu finden
				\item[5] Kann durch Netzwerk-Dumps oder Fuzzing gefunden werden
				\item[9] Schwachstelle ist öffentlich bekannt und kann über Such-Maschinen gefunden werden
				\item[10] Schwachstelle ist direkt auf der Webseite oder in der Adressleiste des Browser zu erkennen
			\end{itemize}
		\end{description}

 Sind Werte für die einzelnen Attribute bestimmt, kann der Score über folgende Formel berechnet werden:\cite{DREADOpenStack}
 \[ 
SCORE_{DREAD} = \frac{DA + R + E + A + DI}{5} 
\]
Dabei entspricht 0 dem geringsten und 10 dem höchsten Risiko.

	\subsubsection{Vergleich von CVSS und DREAD anhand einer XSS-Lücke}
	Im Folgenden wird \textit{CVSS3} mit \textit{DREAD} verglichen. Als Basis gilt eine \textit{Reflected-Cross-Site-Scripting} Lücke, wie sie bereits in einem Beispiel der \textit{FIRST.Org} beschrieben wird.\footnote{\url{https://www.first.org/cvss/examples}}\\
	
	Die Lücke besteht nur in einer bestimmten Version der Webanwendung. Ebenso muss ein valider Datenbankname in den Exploit integriert werden, damit dieser funktioniert. Das System hat die \textit{HTTPOnly}-Flag gesetzt.

	\paragraph{CVSS3}\label{ref:VerglCVSS3}
	Die \textit{CVSS3}-Methodik würde die Schwachstelle wie folgt bewerten:
	
	\begin{description}
		\item[Attack Vector: Network] Die Schwachstelle kann über das Netzwerk erreicht werden.
		\item[Attack Complexity: Low] Auch wenn der Angreifer vor dem Angriff eine kurze Aufklärungsphase erfordert, ist der Angriff aufgrund von Standard-Datenbanknamen einfach durchführbar.
		
		\item[Privileges Required: None] Ein Angreifer braucht keine besonderen Berechtigungen.
		
		\item[User Interaction: Required] Damit der Angriff erfolgreich ist, muss das Opfer eine Aktion ausführen, zum Beispiel das Klicken auf einen manipulierten Link.
		
		\item[Scope: Changed] Es gibt eine Veränderung im \textit{Scope}, da die Lücke zwar in der Webapplikation ist, aber der Angriff im Browser des Users Wirkung zeigt. 
		
		\item[Confidentiality Impact: Low] Informationen aus dem Browser des Opfers können abgegriffen und an den Angreifer geschickt werden. Da aber die Cookies aufgrund der \textit{HTTP-Only}-Flag ausgeschlossen sind, ist der \textit{Impact} nur niedrig.
		
		\item[Integrity Impact: Low] Es können zwar Informationen im Web-Browser des Opfers verändert werden, jedoch nur solche, welche in Verbindung mit der verwundbaren Webapplikation stehen.
		
		\item[Availability Impact: None] Zwar könnte über bösartigen Code der Browser des Users verlangsamt werden, jedoch kann der User den Browser jederzeit beenden.
	\end{description}	
	
	Daraus ergibt sich der \textit{CVSS3-String} \textit{CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:L/I:L/A:N} und ein Rating von 6.1.
	
	\paragraph{DREAD}
	Unter \textit{DREAD} gibt es leider keine Beispiele für eine \textit{Reflected-Cross-Site-Scripting} Lücke. Daher wurden die Werte durch den Autor gewählt.
	
	\begin{description}
		\item[Damage: 3] Die auf der Webseite angezeigten Daten können gelesen und manipuliert werden. Aufgrund des \textit{HTTPOnly}-Flags des Cookies kann die Session jedoch nicht einfach übernommen werden.
		\item[Reproduceability: 10] Der Angriff kann mit einem Web-Browser jederzeit reproduziert werden.
		\item[Exploitability: 7] Der Angriff benötigt einen gültigen Datenbank-Namen. Da es viele Datenbanken mit Standardnamen gibt, ist der Exploit jedoch weiterhin relativ einfach. Zusätzlich muss ein registrierter User im angemeldeten Zustand auf einen manipulierten Link des Angreifers klicken.
		\item[Affected Users: 10] Da der Angriff auf alle User anwendbar ist, sind auch alle User betroffen.
		\item[Discoverbility: 8] Es muss auf die Version der Webanwendung geprüft werden.
	\end{description}
	
	Daraus ergibt sich ein Score von 7.6.
	
	\paragraph{Fazit}
	Der Score von \textit{DREAD} ist mit einem Score von 7.6 um 1.5 Punkte höher als \textit{CVSS3} und muss als "`Hoch"' gewertet werden. Dies beruht darauf, dass die Attribute \textit{Reproduceability} und \textit{Affected Users} auf 10 gesetzt werden müssen. Für eine nur mittel-triviale \textit{XSS}-Lücke ohne die Möglichkeit zur Session-Übernahme scheint dies etwas zu hoch. Der \textit{CVSS3}-Score mit 6.1 scheint als mittleres Finding durchaus angemessen. Dies ist jedoch nur ein Beispiel, für andere Schwachstellen ist es durchaus möglich, dass \textit{DREAD} eine besser passende Klassifizierung darstellt. Im Endeffekt sollte nur konsistent ein System für Pen-Tests eingesetzt werden, damit die Findings vergleichbar bleiben.
			
	\subsection{Dokumentation}
	Ein weiterer wichtiger Punkt während der Durchführung eines Pen-Tests ist die fortlaufende Dokumentation. So sollten sowohl Zwischenergebnisse bezüglich Schwachstellen sowie alle durchgeführten Aktionen dokumentiert werden. Im Folgenden werden verschiedene Methoden vorgestellt, welche diese Dokumentation vereinfachen.
	
		\subsubsection{Netzwerkverkehr}
		Eine der effektivsten Methoden zur Dokumentation bei Web-Applikation und Web-Service Pen-Tests ist das Aufzeichnen des Netzwerkverkehrs. So können jegliche am Server passierenden Aktionen später über den Netzwerkdump einer Aktion des Pen-Testers zugeordnet werden. Dies ist gerade bei Ausfällen der getesteten Anwendung äußerst hilfreich.\\
		
		Technisch kann die Aufzeichnung über mehrere Programme leicht realisiert werden. So kann sowohl \textit{Wireshark}\footnote{\url{https://www.wireshark.org/}} wie auch \textit{TCPDump}\footnote{\url{http://www.tcpdump.org/}} den gesamten Verkehr eines oder mehrerer Netzwerkadapter aufzeichnen und als \textit{PCAP}\footnote{\url{http://xml2rfc.tools.ietf.org/cgi-bin/xml2rfc.cgi?url=https://raw.githubusercontent.com/pcapng/pcapng/master/draft-tuexen-opsawg-pcapng.xml&modeAsFormat=txt/pdf&type=ascii}} speichern.\\
		
		Eine Problemstellung für den Pen-Test stellt eine geschützte \textit{TLS}/\textit{SSL}-Verbindung dar. Da normalerweise der \textit{Private-Key} des Zielsystems nicht zur Verfügung steht, muss die Verbindung unterbrochen werden. Dies kann über \textit{MITMProxy}\footnote{\url{https://mitmproxy.org/}} bewerkstelligt werden, welche einen Proxy-Server aufbaut. Dieser Proxy-Server baut zum Ziel eine \textit{TLS}-Verbindung auf, stellt jedoch zum Pen-Tester eine separate \textit{TLS}-Verbindung auf, für welche ein privates Zertifikat hinterlegt werden kann. Zeichnet man nun den Netzwerkverkehr zwischen dem Pen-Tester und \textit{MITMProxy} auf, können die \textit{TLS}-Verbindungen später mit dem privaten Zertifikat (zum Beispiel in \textit{Wireshark}) entschlüsselt und gesichtet werden.
		
		\subsubsection{Konsoleneingaben}
		Zusätzlich zu Netzwerkdumps kann eine Aufzeichnung der Terminal-Sessions während des Pen-Tests sinnvoll sein. So kann die Ausgabe von Konsolen-Kommandos auch in dem Fall, dass die Bedeutung einer Ausgabe erst später klar wird, einfach in die Dokumentation aufgenommen werden. Um das Transkript zu erstellen, kann das Linux-Tool \textit{screen}\footnote{\url{http://man7.org/linux/man-pages/man1/script.1.html}} verwendet werden. Ein Beispiel-Aufruf wäre:
\lstset{language=bash}
\begin{lstlisting}
script "$(date +"%Y-%m-%d %H:%M:%S").log" -t 2> "$(date +"%Y-%m-%d %H:%M:%S").time"
\end{lstlisting}
Die erstellten Dateien können anschließend über 
\begin{lstlisting}
scriptreplay -t timestamp.time timestamp.log
\end{lstlisting}
abgespielt werden.
		
		\subsubsection{Findings}
		Neben Netzwerkverkehr und Konsoleneingaben sollten auch Findings, sowie Hinweise darauf, möglichst ohne zeitliche Verzögerung dokumentiert werden. Dazu sollte eine standardisierte Form gefunden werden. Wie diese genau aussieht, kann den Pen-Testern überlassen werden. Ein Beispiel wäre ein \textit{Excel-Sheet} mit den wichtigsten Spalten (Name, Kategorie, Beschreibung, \textit{CVSS3}), da diese Infos dort ohne viel Aufwand eingetragen werden können. Zusätzlich sollten Findings, sowie auch schon Hinweise auf mögliche Schwachstellen, umgehend mit einem Screenshot dokumentiert werden. Auf Linux-Laptops können Screenshots unabhängig vom Windows-Manager (solange ein \textit{X-Server} läuft) über das \textit{import}-Kommando von \textit{ImageMagick}\footnote{\url{https://www.imagemagick.org/script/import.php}} genommen werden.