\section{Durchführung}
Eine gute Vorbereitung vereinfacht in vielerlei Hinsicht die Durchführung von Pentests, zum Beispiel weil funktionierende Hard- und Software bereits zur Verfügung steht und nicht während dem Pentest Zeit auf Update und Konfiguration verwendet wird. Trotzdem gibt es einige bewährte Vorgehensweisen, welche man in Prozessen festhalten oder auf welche man sich Einigen sollte. 

	\subsection{Kickoff}
	Unmittelbar vor dem Pen-Test sollte ein Kickoff durchgeführt werden. Zu diesem sollten alle verantwortlichen Stellen eingeladen werden (Business-Unit Manager, System-Administrator, den Sicherheitsverantwortlichen des Projekt , evtl. Informationssicherheit und Hosting-Provider) und Kern-Fragen abgesprochen werden.\\
	
	Im Rahmen dieser Arbeit wurden folgende Fragend definiert und mit Ansprechpartnern der Allianz Deutschland AG geprüft.
			
\paragraph{Allgemein}
\begin{itemize}
	\item Wie ist der Projekt-Name?
	\item Wer sind die Teilnehmer?
	\item Wann soll der Test durchgeführt werden?
	\item Was ist das Ziel des Tests?
	\item In welcher Stage befindet sich die Anwendung? Development Test System Integration Produktion
	\item Welche IP-Adressen sollen getested werden?
	\item Welche URLs sollten getestet werden?
	\item Welche Zugangsdaten sollen genutzt werden?
	\item Sollen Denial-Of-Service-Angriffe durchgeführt werden? Ja Nein
	\item Liegt ein Haftungssauschschluss vor? Ja Nein
	\item Liegt eine Erlaubnis des Server-Betreibers vor? Ja Nein
	\item Gibt es eine Deadline für den Bericht? Ja Nein
	\item Erste Ergebnisse am Ende des Pen-Tests in einfacher Form zukommen lassen (z.B. Excel)? Ja Nein
\end{itemize}


\paragraph{Fragen an den System-Administrator}
\begin{itemize}
\item Gibt es Systeme, die als instabil angesehen werden (alte Patch-Stände, Legacy Systeme etc.)? Ja Nein
\item Gibt es Systeme von Dritten, die ausgeschlossen werden müssen oder für die weitere Genehmigungen notwendig sind? Ja Nein
\item Was ist die Durchschnittszeit zur Wiederherstellung der Funktionalität eines Services?
\item Ist eine Monitoring-Software im Einsatz? Ja Nein
\item Welche sind die kritischsten Applikationen?
\item Werden in einem regelmäßigen Turnus Backups erstellt und getestet? Ja Nein
\end{itemize}

\paragraph{Fragen an den Business Unit Manager}
\begin{itemize}
\item Ist die Führungsebene über den Test informiert?
\item Ja Nein
\item Welche Daten stellen das größte Risiko dar, falls diese manipuliert werden?
\item Gibt es Testfälle, die die Funktionalität der Services prüfen und belegen können? Ja Nein
\item Sind "Disaster Recovery Procedures" vorhanden? Ja Nein
\end{itemize}

\paragraph{Abschluss}
\begin{itemize}
\item Offene TODOs
\end{itemize}
	
		Um den Kickoff möglichst effizient zu gestalten, wurden auch diese Fragen über die gleiche Technik wie bei TODO in eine Webanwendung integriert. Diese ist auf dem Datenträger unter TODO abgelegt.
			
	\subsection{Kategorisierung von Findings}
	Um einen besseren Überblick über Findings zu bekommen, werden diese meist in Kategorien eingeteilt. Im Folgenden werden zwei Methoden, die OWASP TOP 10 und die CWE (\textit{Common Weakness Enumeration}), vorgestellt.
	
		\subsubsection{OWASP TOP 10}
		Die \textit{OWASP Foundation} (Open Web Application Security Project) ist ein non-Profit Organisation mit dem Ziel es Unternehmen zu vereinfachen sichere Software zu schreiben.\\
		
		Im Rahmen des Projekts erfasst die OWASP alle 3 Jahre, zuletzt für das Jahr 2013, aktuell für das Jahr 2016, die am meisten aufgetretenen Schwachstellen und veröffentlicht die sogenannte \textit{OWASP TOP 10}. Aus dem Datensatz von 2013 ergeben sich folgende Kategorien\cite{OWASPTOP10}:
		\begin{itemize}
			\item A1-Injection
			\item A2-Broken Authentication and Session Management
			\item A3-Cross-Site Scripting (XSS)
			\item A4-Insecure Direct Object References
			\item A5-Security Misconfiguration
			\item A6-Sensitive Data Exposure
			\item A7-Missing Function Level Access Control
			\item A8-Cross-Site Request Forgery (CSRF)
			\item A9-Using Components with Known Vulnerabilities 
			\item A10-Unvalidated Redirects and Forwards
		\end{itemize}
		
		Diese sind in der Security-Szene weit verbreitet und können gut genutzt werden, um Pen-Test-Findings zu kategorisieren.
		
		\subsubsection{Common Weakness Enumeration}
		Eine Alternative ist die von der MITRE Corperation (mit Unterstützung verschiedener andere Stellen) entwickelte \textit{Common Weakness Enumeration}. Diese ist wesentlich feingranularer als die OWASP TOP 10, so umfasst die Version 2.10 vom 19.01.2017 über 1000 verschiedene Schwachstellen.\footnote{\url{http://cwe.mitre.org/data/published/cwe_v2.10.pdf}}\cite{MITRECWE}\\
		
		Dies hat den Vorteil, dass man die Schwachstellen meist genau einer Kategorie zuweisen kann. Jedoch ist die Zuordnung wesentlich aufwändiger.
			
	\subsection{Bewertung von Findings}
	Für die in einem Pentest gefundenen Findings sollte direkt nach deren Entdeckung eine Bewertung durchgeführt werden. Dies ist notwendig, um entsprechende Eskalationsstufen zu informieren, wenn kritische Findings auftreten. Zur Bewertung bieten sich verschiedene Systeme an, im Folgenden werden \textit{CVSS} und \textit{DREAD} vorgestellt und verglichen.
	
			\subsubsection{CVSS}
			Das CVSS (Common Vulnerability Scoring System) wurde entwickelt von und gehört der FIRST.Org, Inc., einer in Amerika ansässigen wohltätigen Organisation, welche Lösungen zur Koordination und Unterstützung von IT-Security-Teams entwickelt.\\
			
			Es ist ein System, welches Anhand von mehrere Attributen die Charakteristiken und Risiko-Gerade von Schwachstellen bestimmt. Dabei gibt es 3 Metriken: \textit{Base}, \textit{Temporal} und \textit{Environmental}. Die \textit{Base}-Metrik beschreibt den Grund-Score einer Schwachstelle, die \textit{Temporal}-Metrik der Score zum aktuellen Zeitpunkt und die  \textit{Environmental}-Metrik den Score in einem bestimmten Umfeld. Der Base-Score ist verpflichtend auszufüllen, die anderen Metriken können das Ergebnis verfeinern, sind aber nicht zwingend notwendig.\\
			
			Die Base-Metrik ergibt einen Score zwischen 0.0 und 10.0, welcher durch die beiden Zusatz-Metriken erhöht oder geschwächt werden kann. Eine komprimierte Darstellungsweise stellt der CVSS-Vector-String dar, welcher die Attribute aus allen drei Metriken sowie deren jeweiligen Werte komprimiert anzeigt. Die Attribute für die Base-Metrik sind im Vergleich unter \ref{ref:VerglCVSS3} dargestellt. Die Attribute für die \textit{Temporal}- und \textit{Environmental}-Metrik genauen Formeln können der Spezifikation\footnote{\url{https://www.first.org/cvss/specification-document}} entnommen werden.\cite{FIRSTCVSS}\\
		
		Zusätzlich gib es für den Score eine Zuordnung zu 5 gröberen Risikostufen. Diese ist Tabelle \ref{tab:cvssToRisk} zu entnehmen.\\
		
		\begin{table}
			\centering
			\begin{tabularx}{5cm}{l | l}
				Risikostufe & CVSS3-Score \\\hline
				None & 0.0 \\
				Low & 0.1 - 3.9 \\
				Medium & 4.0 - 6.9 \\
				High & 7.0 - 8.9 \\
				Critical & 9.0 - 10.0	 \\
			\end{tabularx}
			\caption{Zuordnung von CVSS3-Score zur Risikostufe
			\label{tab:cvssToRisk}
 \cite{FIRSTCVSSSpec}}
		\end{table}
		
		Ebenso gibt es für CVSS Module für zum Beispiel Python\footnote{\url{https://pypi.python.org/pypi/cvss}}, um CVSS-Vector-Strings auszuwerten und die Scores für die einzelnen Metriken zu berechnen.

		\subsubsection{DREAD}
		
		DREAD wie CVSS ein Metrik-System zur Einschätzung des resultierenden Risikos aus einer Schwachstelle. DREAD ist dabei das Akronym für die fünf bewerteten Attribute der Schwachstelle. Jedes Attribut hat eine Wert von 0 bis 10, wobei 10 immer der Schlimmste anzunehmende Fall ist. Im Folgenden sind die Attribute sowie grobe Richtwerte pro Attribut aufgeführt.\cite{DREADOWASP}
		
		\begin{description}
			\item[Damage: ] Wie viel Schaden würde eine Ausnutzung der Schwachstelle bedeuten?
			\begin{itemize}
				\item[0] Kein Schaden
				\item[5] Die Daten eines einzelnen Users sind betroffen
				\item[10] Komplette Zerstörung der Daten oder des Systems 
			\end{itemize}
			
			\item[Reproducibility: ] Wie verlässlich funktioniert der Exploit?
			\begin{itemize}
				\item[0] Selbst mit erhöhten Rechten ist ein funktionierender Exploit äußerst unwahrscheinlich
				\item[5] Mehrstufiges Vorgehen notwendig, es gibt vorgefertigte Scripte oder Tools
				\item[10] Nicht authentifizierte User können den Exploit trivial ohne Hilfsmittel reproduzierbar durchführen
			\end{itemize}
			
			\item[Exploitability: ] Wie schwer ist es die Schwachstelle auszunutzen?
			\begin{itemize}
				\item[0] Darf nicht vergeben werden, es wird angenommen das jede Schwachstelle mit genügend Aufwand exploitable ist
				\item[1] Selbst mit direktem Wissen der Schwachstelle gibt derzeit keine bekannte Methode zur Ausnutzung
				\item[5] Der Exploit ist öffentlich, mittleres Können durch den Angreifer benötigt, Angreifer muss authentisiert sein
				\item[7] Der Exploit ist öffentlich, Angreifer muss nicht authentisiert sein
				\item[10] Triviale Ausnutzung, zum Beispiel über einen Webbrowser
			\end{itemize}
			
			\item[Affected Users: ] Wie viele Nutzer betrifft die Schwachstelle?
			\begin{itemize}
				\item[0] Keine User betroffen
				\item[5] Einige User betroffen, aber nicht alle
				\item[10] Alle User betroffen
			\end{itemize}
			
			\item[Discoverability: ] Wie einfach ist die Schwachstelle zu finden?
			\begin{itemize}
				\item[0] Selbst mit Source-Code-Zugriff und erhöhten Rechten schwer zu finden
				\item[5] Kann durch Netzwerk-Dumps oder Fuzzing gefunden werden
				\item[9] Schwachstelle ist öffentlich bekannt und kann über Such-Maschinen gefunden werden
				\item[10] Schwachstelle ist direkt auf der Webseite oder in der Adressleiste des Browser zu erkennen
			\end{itemize}
		\end{description}

 Sind Werte für die einzelnen Attribute bestimmt, kann der Score über folgende Formel berechnet werden:\cite{DREADOpenStack}
 \[ 
SCORE_{DREAD} = \frac{DA + R + E + A + DI}{5} 
\]

	\subsubsection{Vergleich von CVSS und DREAD anhand einer XSS-Lücke}
	Im Folgenden wird CVSS3 mit DREAD verglichen. Als Basis gilt eine Reflected-Cross-Site-Scripting Lücke, wie Sie bereits in einem Beispiel FIRST.Org beschrieben wird.\footnote{\url{https://www.first.org/cvss/examples}}\\
	
	Die Lücke besteht nur in einer bestimmten Version der Webanwendung. Ebenso muss ein valider Datenbankname in den Exploit integriert werden, damit dieser funktioniert. Das System hat die \textit{HTTPOnly}-Flag gesetzt.

	\paragraph{CVSS3}\label{ref:VerglCVSS3}
	Die CVSS3-Methodik würde die Vulnerability wie folgt bewerten:
	
	\begin{description}
		\item[Attack Vector: Network] Die Schwachstelle kann über das Netzwerk erreicht werden.
		\item[Attack Complexity: Low] Although an attacker needs to perform some reconnaissance of the target system, a valid session token can be easily obtained and many systems likely use well-known or default database names.
		\item[Privileges Required: None] An attacker requires no privileges to mount an attack.
		\item[User Interaction: Required] A successful attack requires the victim to visit the vulnerable component, e.g. by clicking a malicious URL.
		\item[Scope: Changed] The vulnerable component is the web server running the phpMyAdmin software. The impacted component is the victim's browser. 
		\item[Confidentiality Impact: Low] Information maintained in the victim's web browser can be read and sent to the attacker. This is constrained to information associated with the web site running phpMyAdmin, and cookie data is excluded because the HttpOnly flag is enabled by default by phpMyAdmin. If the HttpOnly flag is not set, the Confidentiality Impact will become High if the attacker has access to sufficient cookie data to hijack the victim's session.
		\item[Integrity Impact: Low] Information maintained in the victim's web browser can be modified, but only information associated with the web site running phpMyAdmin.
		\item[Availability Impact: None] The malicious code can deliberately slow the victim's system, but the effect is usually minor and the victim can easily close the browser tab to terminate it.
	\end{description}	
	
	Daraus ergibt sich der CVSS3-String \textit{CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:L/I:L/A:N} und ein Rating von 6.1.
	
	\paragraph{DREAD}
	Unter DREAD gibt es leider keine Beispiele für eine Reflected-Cross-Site-Scripting Lücke. Daher sind die Werte unten selbst gewählt.
	
	\begin{description}
		\item[Damage: 3] Die auf der Webseite angezeigten Daten können gelesen und manipuliert werden. Aufgrund des \textit{HTTPOnly}-Flags des Cookies kann die Session jedoch nicht einfach übernommen werden.
		\item[Reproduceability: 10] Der Angriff kann mit einem Web-Browser jederzeit reproduziert werden.
		\item[Exploitability: 7] Der Angriff benötigt einen gültigen Datenbank-Namen. Da es viele Datenbaken mit Standardnamen gibt, ist der Exploit jedoch weiterhin relativ einfach. Zusätzlich muss ein registrierter User im angemeldeten Zustand auf einen manipulierten Link des Angreifers klicken.
		\item[Affected Users: 10] Alle User sind betroffen.
		\item[Discoverbility: 8] Es muss auf die Version der Webanwendung geprüft werden.
	\end{description}
	
	Daraus ergibt sich ein Score von 7.6.
	
	\paragraph{Fazit}
	Der Score von DREAD ist mit einem Score von 7.6 um 1.5 Punkte höher als CVSS3 und muss als "`Hoch"' gewertet werden. Dies beruht darauf, dass die Attribute \textit{Reproduceability} und \textit{Affected Users} auf 10 gesetzt werden müssen. Für eine nur mittel-triviale XSS-Lücke ohne die Möglichkeit zur Session-Übernahme scheint dies etwas zu hoch. Der CVSS3-Score mit 6.1 scheint als mittleres Finding durchaus angemessen. Dies ist jedoch nur ein Beispiel, für andere Schwachstellen ist es durchaus möglich, dass DREAD eine besser passende Klassifizierung darstellt. Im Endeffekt sollte nur Konsistent ein System für Pentests eingesetzt werden, damit die Findings vergleichbar bleiben.
			
	\subsection{Dokumentation}
	Ein weitere wichtiger Punkt während der Durchführung eines Pentests ist die fortlaufende Dokumention. So sollten sowohl Fortschritte bezüglich Schwachstellen sowie alle durchgeführten Aktionen dokumentiert werden. Im Folgenden werden verschiedene Methoden vorgestellt, welche diese Dokumention vereinfachen.
	
		\subsubsection{Netzwerkverkehr}
		Eine der effektivsten Methoden zur Dokumention bei Web-Applikation und Web-Service Pentests ist das Aufzeichnen des Netzwerkverkehrs. So können jegliche am Server passierenden Aktionen später über den Netzwerkdump einer Aktion des Pentesters zugeordnet werden. Dies ist gerade bei Ausfällen der getesteten Anwendung äußerst hilfreich.\\
		
		Technisch kann die Aufzeichnung über mehrere Programme leicht realisiert werden. So kann sowohl Wireshark\footnote{\url{https://www.wireshark.org/}} wie auch TCPDump\footnote{\url{http://www.tcpdump.org/}} den gesamten Verkehr eines oder mehrerer Netzweradapter aufzeichnen und als PCAP speichern.\\
		
		Ein Problem ist, wenn die Verbindung über TLS/SSL geschützt ist. Da normalerweise der \textit{Private-Key} des Zielsystems nicht zur Verfügung steht, muss die Verbindung unterbrochen werden. Dies kann über MITMProxy\footnote{\url{https://mitmproxy.org/}} bewerkstelligt werden, welche einen Proxy-Server aufbaut. Dieser Proxy-Server baut zum Ziel eine TLS-Verbindung auf, stellt jedoch zum Pentester eine separate TLS-Verbindung auf, für welche ein privates Zertifikat hinterlegt werden kann. Zeichnet man nun den Netzwerkverkehr zwischen dem Pentester und MITMProxy auf, können die TLS-Verbindungen später mit dem privaten Zertifikat (zum Beispiel in Wireshark) decrypted und gesichtet werden.
		
		\subsubsection{Konsoleneingaben}
		Zusätzlich zu Netzwerkdumps kann eine Aufzeichnung der Terminal-Sessions während des Pentests sinnvoll sein. So kann die Ausgabe von Konsolen-Kommandos auch in dem Fall, dass die Bedeutung einer Ausgabe erst später klar wird, einfach in die Dokumentation aufnehmen. Um das Transkript zu erstellen, kann das Linux-Tool \textit{screen}\footnote{\url{http://man7.org/linux/man-pages/man1/script.1.html}} verwendet werden. Ein Beispiel-Aufruf wäre:
\lstset{language=bash}
\begin{lstlisting}
script "$(date +"%Y-%m-%d %H:%M:%S").log" -t 2> "$(date +"%Y-%m-%d %H:%M:%S").time"
\end{lstlisting}
Die erstellen Dateien können anschließend über 
\begin{lstlisting}
scriptreplay -t timestamp.time timestamp.log
\end{lstlisting}
abgespielt werden.
		
		\subsubsection{Findings}
		Neben Netzwerkverkehr und Konsoleneingaben sollten auch Findings, sowie Hinweise darauf, möglichst umgehend dokumentiert werden. Dazu sollte eine standardisierte Form gefunden werden. Wie diese genau aussieht, kann den Penstestern überlassen werden. Ein Beispiel wäre ein Excel-Sheet mit den wichtigsten Spalten (Name, Kategorie, Beschreibung, CVSS3), da diese Infos dort ohne viel Aufwand eingetragen werden können. Zusätzlich sollten Findings, sowie auch schon Hinweise auf mögliche Schwachstellen, umgehend mit einem Screenshot dokumentiert werden. Auf Linux-Laptops können Screenshots unabhängig vom Windows-Manager (solange ein X-Server läuft) über das \textit{import}-Kommando von ImageMagick\footnote{\url{https://www.imagemagick.org/script/import.php}} genommen werden.